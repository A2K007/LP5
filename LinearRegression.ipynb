{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "EP0ypjuZXRTw"
      },
      "outputs": [],
      "source": [
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "vIsqyxlGXpGf"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv(\"1_boston_housing.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "DGDGC6A4XrQv",
        "outputId": "795e360e-2fa9-4942-de49-d75b56d6ea03"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>crim</th>\n",
              "      <th>zn</th>\n",
              "      <th>indus</th>\n",
              "      <th>chas</th>\n",
              "      <th>nox</th>\n",
              "      <th>rm</th>\n",
              "      <th>age</th>\n",
              "      <th>dis</th>\n",
              "      <th>rad</th>\n",
              "      <th>tax</th>\n",
              "      <th>ptratio</th>\n",
              "      <th>b</th>\n",
              "      <th>lstat</th>\n",
              "      <th>MEDV</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.00632</td>\n",
              "      <td>18.0</td>\n",
              "      <td>2.31</td>\n",
              "      <td>0</td>\n",
              "      <td>0.538</td>\n",
              "      <td>6.575</td>\n",
              "      <td>65.2</td>\n",
              "      <td>4.0900</td>\n",
              "      <td>1</td>\n",
              "      <td>296</td>\n",
              "      <td>15.3</td>\n",
              "      <td>396.90</td>\n",
              "      <td>4.98</td>\n",
              "      <td>24.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.02731</td>\n",
              "      <td>0.0</td>\n",
              "      <td>7.07</td>\n",
              "      <td>0</td>\n",
              "      <td>0.469</td>\n",
              "      <td>6.421</td>\n",
              "      <td>78.9</td>\n",
              "      <td>4.9671</td>\n",
              "      <td>2</td>\n",
              "      <td>242</td>\n",
              "      <td>17.8</td>\n",
              "      <td>396.90</td>\n",
              "      <td>9.14</td>\n",
              "      <td>21.6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.02729</td>\n",
              "      <td>0.0</td>\n",
              "      <td>7.07</td>\n",
              "      <td>0</td>\n",
              "      <td>0.469</td>\n",
              "      <td>7.185</td>\n",
              "      <td>61.1</td>\n",
              "      <td>4.9671</td>\n",
              "      <td>2</td>\n",
              "      <td>242</td>\n",
              "      <td>17.8</td>\n",
              "      <td>392.83</td>\n",
              "      <td>4.03</td>\n",
              "      <td>34.7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.03237</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.18</td>\n",
              "      <td>0</td>\n",
              "      <td>0.458</td>\n",
              "      <td>6.998</td>\n",
              "      <td>45.8</td>\n",
              "      <td>6.0622</td>\n",
              "      <td>3</td>\n",
              "      <td>222</td>\n",
              "      <td>18.7</td>\n",
              "      <td>394.63</td>\n",
              "      <td>2.94</td>\n",
              "      <td>33.4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.06905</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.18</td>\n",
              "      <td>0</td>\n",
              "      <td>0.458</td>\n",
              "      <td>7.147</td>\n",
              "      <td>54.2</td>\n",
              "      <td>6.0622</td>\n",
              "      <td>3</td>\n",
              "      <td>222</td>\n",
              "      <td>18.7</td>\n",
              "      <td>396.90</td>\n",
              "      <td>5.33</td>\n",
              "      <td>36.2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>501</th>\n",
              "      <td>0.06263</td>\n",
              "      <td>0.0</td>\n",
              "      <td>11.93</td>\n",
              "      <td>0</td>\n",
              "      <td>0.573</td>\n",
              "      <td>6.593</td>\n",
              "      <td>69.1</td>\n",
              "      <td>2.4786</td>\n",
              "      <td>1</td>\n",
              "      <td>273</td>\n",
              "      <td>21.0</td>\n",
              "      <td>391.99</td>\n",
              "      <td>9.67</td>\n",
              "      <td>22.4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>502</th>\n",
              "      <td>0.04527</td>\n",
              "      <td>0.0</td>\n",
              "      <td>11.93</td>\n",
              "      <td>0</td>\n",
              "      <td>0.573</td>\n",
              "      <td>6.120</td>\n",
              "      <td>76.7</td>\n",
              "      <td>2.2875</td>\n",
              "      <td>1</td>\n",
              "      <td>273</td>\n",
              "      <td>21.0</td>\n",
              "      <td>396.90</td>\n",
              "      <td>9.08</td>\n",
              "      <td>20.6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>503</th>\n",
              "      <td>0.06076</td>\n",
              "      <td>0.0</td>\n",
              "      <td>11.93</td>\n",
              "      <td>0</td>\n",
              "      <td>0.573</td>\n",
              "      <td>6.976</td>\n",
              "      <td>91.0</td>\n",
              "      <td>2.1675</td>\n",
              "      <td>1</td>\n",
              "      <td>273</td>\n",
              "      <td>21.0</td>\n",
              "      <td>396.90</td>\n",
              "      <td>5.64</td>\n",
              "      <td>23.9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>504</th>\n",
              "      <td>0.10959</td>\n",
              "      <td>0.0</td>\n",
              "      <td>11.93</td>\n",
              "      <td>0</td>\n",
              "      <td>0.573</td>\n",
              "      <td>6.794</td>\n",
              "      <td>89.3</td>\n",
              "      <td>2.3889</td>\n",
              "      <td>1</td>\n",
              "      <td>273</td>\n",
              "      <td>21.0</td>\n",
              "      <td>393.45</td>\n",
              "      <td>6.48</td>\n",
              "      <td>22.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>505</th>\n",
              "      <td>0.04741</td>\n",
              "      <td>0.0</td>\n",
              "      <td>11.93</td>\n",
              "      <td>0</td>\n",
              "      <td>0.573</td>\n",
              "      <td>6.030</td>\n",
              "      <td>80.8</td>\n",
              "      <td>2.5050</td>\n",
              "      <td>1</td>\n",
              "      <td>273</td>\n",
              "      <td>21.0</td>\n",
              "      <td>396.90</td>\n",
              "      <td>7.88</td>\n",
              "      <td>11.9</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>506 rows Ã— 14 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "        crim    zn  indus  chas    nox     rm   age     dis  rad  tax  \\\n",
              "0    0.00632  18.0   2.31     0  0.538  6.575  65.2  4.0900    1  296   \n",
              "1    0.02731   0.0   7.07     0  0.469  6.421  78.9  4.9671    2  242   \n",
              "2    0.02729   0.0   7.07     0  0.469  7.185  61.1  4.9671    2  242   \n",
              "3    0.03237   0.0   2.18     0  0.458  6.998  45.8  6.0622    3  222   \n",
              "4    0.06905   0.0   2.18     0  0.458  7.147  54.2  6.0622    3  222   \n",
              "..       ...   ...    ...   ...    ...    ...   ...     ...  ...  ...   \n",
              "501  0.06263   0.0  11.93     0  0.573  6.593  69.1  2.4786    1  273   \n",
              "502  0.04527   0.0  11.93     0  0.573  6.120  76.7  2.2875    1  273   \n",
              "503  0.06076   0.0  11.93     0  0.573  6.976  91.0  2.1675    1  273   \n",
              "504  0.10959   0.0  11.93     0  0.573  6.794  89.3  2.3889    1  273   \n",
              "505  0.04741   0.0  11.93     0  0.573  6.030  80.8  2.5050    1  273   \n",
              "\n",
              "     ptratio       b  lstat  MEDV  \n",
              "0       15.3  396.90   4.98  24.0  \n",
              "1       17.8  396.90   9.14  21.6  \n",
              "2       17.8  392.83   4.03  34.7  \n",
              "3       18.7  394.63   2.94  33.4  \n",
              "4       18.7  396.90   5.33  36.2  \n",
              "..       ...     ...    ...   ...  \n",
              "501     21.0  391.99   9.67  22.4  \n",
              "502     21.0  396.90   9.08  20.6  \n",
              "503     21.0  396.90   5.64  23.9  \n",
              "504     21.0  393.45   6.48  22.0  \n",
              "505     21.0  396.90   7.88  11.9  \n",
              "\n",
              "[506 rows x 14 columns]"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Print the Dataset\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "za2HRFLUXvlB"
      },
      "outputs": [],
      "source": [
        "# Column Names and Description :\n",
        "\n",
        "# crim : per capita crime rate by town\n",
        "# zn : proportion of residential land zoned for over 25000 sqft\n",
        "# indus : proportion of non-retail business acres per town\n",
        "# chas : 1 if tract bounds river, 0 otherwise\n",
        "# nox : nitric oxide concentration ppm\n",
        "# rm : average number of rooms per dwelling\n",
        "# age : proportion of owner houses built before 1940\n",
        "# dis : weighted distances to 5 Boston employment centres\n",
        "# rad : accessibility to radial highways\n",
        "# tax : property tax rate\n",
        "# ptratio : pupil-teacher ratio in the town\n",
        "# b : proportion of black people in the town\n",
        "# lstat : % lower status of population\n",
        "# medv : median value price of homes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t5XFF6hOXzLk",
        "outputId": "5fdecbc4-c581-4fd9-87b3-b476bb9d68cd"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "crim       0\n",
              "zn         0\n",
              "indus      0\n",
              "chas       0\n",
              "nox        0\n",
              "rm         0\n",
              "age        0\n",
              "dis        0\n",
              "rad        0\n",
              "tax        0\n",
              "ptratio    0\n",
              "b          0\n",
              "lstat      0\n",
              "MEDV       0\n",
              "dtype: int64"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Check for Null Values\n",
        "\n",
        "df.isnull().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zlcHgp8dX3n9",
        "outputId": "abee2d38-16ab-4f94-95bd-fc6698c2eb66"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "lstat     -0.737663\n",
              "ptratio   -0.507787\n",
              "indus     -0.483725\n",
              "tax       -0.468536\n",
              "nox       -0.427321\n",
              "crim      -0.388305\n",
              "rad       -0.381626\n",
              "age       -0.376955\n",
              "chas       0.175260\n",
              "dis        0.249929\n",
              "b          0.333461\n",
              "zn         0.360445\n",
              "rm         0.695360\n",
              "MEDV       1.000000\n",
              "Name: MEDV, dtype: float64"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Observe Co-relation between input features and the output variable MEDV\n",
        "# Positive Co-relation indicates that value of a variable increases as another increases\n",
        "# Negative Co-relation indicates that value of a variable decreases as another increases and vice-versa\n",
        "\n",
        "df.corr()['MEDV'].sort_values()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "PYRzgy6WYSXT"
      },
      "outputs": [],
      "source": [
        "# lstat, ptratio and rm are the features that significantly influence the output variable\n",
        "# However, the results when only these 3 are considered, vs when all features are considered are not drastically different"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "5aZGu7a-YWDD"
      },
      "outputs": [],
      "source": [
        "# X = df.loc[:,['lstat', 'ptratio', 'rm']] in case you want to consider only the high impact features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "iHWXXiNUYX4l"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X = df.loc[:,df.columns!='MEDV']\n",
        "Y = df.loc[:,df.columns=='MEDV']\n",
        "\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X,Y,test_size=0.25,random_state=123)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "EB5rCdlYYb1z"
      },
      "outputs": [],
      "source": [
        "# MinMax Scaling is a technique used for Normalization,\n",
        "# So that all the features fall in a specified range, typically from 0 to 1\n",
        "# The formula is as follows :\n",
        "# X = (X-Xmin)/(Xmax-Xmin)\n",
        "\n",
        "# StandardScaling is another Normalization technique,\n",
        "# It scales the data such that mean of each feature is 0, and the variance is 1\n",
        "# Since linear regression works well when the featues are normally distributed,\n",
        "# Robustness to outliers, and preservation of shape of the distribution,\n",
        "# StandardScaling might be a better normalization technique in our current use case for the given dataset and linear regression\n",
        "\n",
        "# The formula is as follows :\n",
        "# Xscaled = (X-u)/Ïƒ\n",
        "# Where u is the mean and Ïƒ is the standard deviation\n",
        "\n",
        "# Hence, here we use StandardScaling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "Imh__HzZYkUZ"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "scaler = StandardScaler()\n",
        "\n",
        "scaler.fit(X_train)\n",
        "\n",
        "X_train = scaler.transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Why do we fit only the X_train and not the X_test?\n",
        "# We use x_train only for fitting the data,\n",
        "# To generalize rules that we are going to use when we transform both training and testing data\n",
        "# Having a peek at X_test to fit defeats the purpose of testing data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xsZoofDLYmLU",
        "outputId": "bbd6af5a-fb13-4587-edc8-0712eaa0c44f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From c:\\Users\\avish\\anaconda3\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
            "\n",
            "WARNING:tensorflow:From c:\\Users\\avish\\anaconda3\\Lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From c:\\Users\\avish\\anaconda3\\Lib\\site-packages\\keras\\src\\optimizers\\__init__.py:309: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " layer1 (Dense)              (None, 128)               1792      \n",
            "                                                                 \n",
            " layer2 (Dense)              (None, 64)                8256      \n",
            "                                                                 \n",
            " output_layer (Dense)        (None, 1)                 65        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 10113 (39.50 KB)\n",
            "Trainable params: 10113 (39.50 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "# A sequential model, where output of one layer is the input for the next layer\n",
        "\n",
        "from tensorflow.keras.layers import Dense\n",
        "# Dense is a type of layer in which reprents a fully connected layer,\n",
        "# Where each neuron in layer receives input from every neuron in previous layer\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "model.add(Dense(128, input_shape=(13,), activation = 'relu', name = 'layer1'))\n",
        "# Dense is the type of layer,\n",
        "# Parameters are number of neurons, input features, activation function and name of the layer\n",
        "# relu activation function -> rectified linear unit\n",
        "# f(x) = max(0,x)\n",
        "# Introduces non linearity in the network, to learn about complex relationships within the data\n",
        "\n",
        "model.add(Dense(64,activation='relu',name = 'layer2'))\n",
        "model.add(Dense(1,activation ='linear',name ='output_layer'))\n",
        "\n",
        "# We have only 1 neuron in output layer,that too with linear activation function for our regression task\n",
        "# linear activation function > f(x) = x\n",
        "# Essential so that we obtain only continuous output for our regression\n",
        "\n",
        "model.compile(optimizer='adam',loss='mse',metrics=['mae'])\n",
        "# adam optimizer uses adaptive learning ie its learning rate changes during training\n",
        "# Also utilizes the 'momentum' concept to adjust the gradient descent\n",
        "# adam optimizer is used to adjust the weights such that the loss metric 'mse' is minimized during training\n",
        "# The 'mae' metric will be observed at each training epoch\n",
        "\n",
        "model.summary()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "vGNAINVqZqSI"
      },
      "outputs": [],
      "source": [
        "# The above provides the model summary,\n",
        "# Parameters indicate the total weights and biases in each layer\n",
        "# In layer 1,\n",
        "# We have 13(inputs)*128(weights)+128 bias for each neuron = 1792 paramters\n",
        "# For layer 2,\n",
        "# We have 128(inputs)*64(weights)+64 biases for each neuron = 8256 paramteres\n",
        "# Recall that it is a fully connected dense layer.\n",
        "\n",
        "# Trainable parameters are the parameters that will be updated by backpropagation during training\n",
        "# Non-trainable parameters are the ones that will not be updated.\n",
        "\n",
        "# Now our model is defined and compiled,we have also given a summary of the model's architecture.\n",
        "# It is ready to be trained on data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vaBtxHVbaDjw",
        "outputId": "157bff8c-25a0-481b-ff0b-1bc65a8211a4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "WARNING:tensorflow:From c:\\Users\\avish\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
            "\n",
            "WARNING:tensorflow:From c:\\Users\\avish\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
            "\n",
            "12/12 [==============================] - 1s 21ms/step - loss: 577.0314 - mae: 22.0942 - val_loss: 563.3374 - val_mae: 21.7714\n",
            "Epoch 2/100\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 500.1946 - mae: 20.3052 - val_loss: 486.0005 - val_mae: 20.0573\n",
            "Epoch 3/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 412.6765 - mae: 18.1247 - val_loss: 389.6488 - val_mae: 17.7252\n",
            "Epoch 4/100\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 307.8892 - mae: 15.2565 - val_loss: 274.2816 - val_mae: 14.4625\n",
            "Epoch 5/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 195.0520 - mae: 11.6276 - val_loss: 165.1079 - val_mae: 10.5752\n",
            "Epoch 6/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 109.8460 - mae: 8.2614 - val_loss: 90.1847 - val_mae: 7.6266\n",
            "Epoch 7/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 69.9084 - mae: 6.4132 - val_loss: 56.3582 - val_mae: 5.7759\n",
            "Epoch 8/100\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 51.1988 - mae: 5.4702 - val_loss: 39.1412 - val_mae: 4.8722\n",
            "Epoch 9/100\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 37.8636 - mae: 4.6050 - val_loss: 29.8871 - val_mae: 4.2022\n",
            "Epoch 10/100\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 29.3687 - mae: 3.9763 - val_loss: 23.6604 - val_mae: 3.6679\n",
            "Epoch 11/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 25.3581 - mae: 3.6327 - val_loss: 20.2224 - val_mae: 3.2993\n",
            "Epoch 12/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 23.1109 - mae: 3.4908 - val_loss: 17.9630 - val_mae: 3.1255\n",
            "Epoch 13/100\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 21.8357 - mae: 3.4040 - val_loss: 17.1123 - val_mae: 3.0709\n",
            "Epoch 14/100\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 20.5627 - mae: 3.2883 - val_loss: 16.4272 - val_mae: 2.9822\n",
            "Epoch 15/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 19.7770 - mae: 3.2240 - val_loss: 15.6493 - val_mae: 2.9394\n",
            "Epoch 16/100\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 18.6344 - mae: 3.1287 - val_loss: 15.1567 - val_mae: 2.9098\n",
            "Epoch 17/100\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 17.8552 - mae: 3.0378 - val_loss: 15.0008 - val_mae: 2.9478\n",
            "Epoch 18/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 17.2722 - mae: 2.9813 - val_loss: 14.2431 - val_mae: 2.9134\n",
            "Epoch 19/100\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 16.6062 - mae: 2.9128 - val_loss: 13.5653 - val_mae: 2.8460\n",
            "Epoch 20/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 16.0575 - mae: 2.8902 - val_loss: 12.6461 - val_mae: 2.7387\n",
            "Epoch 21/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 15.4274 - mae: 2.8358 - val_loss: 12.2145 - val_mae: 2.7418\n",
            "Epoch 22/100\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 14.8932 - mae: 2.7973 - val_loss: 11.7259 - val_mae: 2.6219\n",
            "Epoch 23/100\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 14.4689 - mae: 2.7614 - val_loss: 11.4939 - val_mae: 2.5979\n",
            "Epoch 24/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 13.9836 - mae: 2.6800 - val_loss: 10.9252 - val_mae: 2.5607\n",
            "Epoch 25/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 13.5641 - mae: 2.6447 - val_loss: 10.4983 - val_mae: 2.4744\n",
            "Epoch 26/100\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 13.2517 - mae: 2.6056 - val_loss: 10.1305 - val_mae: 2.4081\n",
            "Epoch 27/100\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 12.9646 - mae: 2.5713 - val_loss: 9.5380 - val_mae: 2.2833\n",
            "Epoch 28/100\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 12.5864 - mae: 2.5514 - val_loss: 9.2287 - val_mae: 2.2762\n",
            "Epoch 29/100\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 12.3203 - mae: 2.5058 - val_loss: 8.8821 - val_mae: 2.2017\n",
            "Epoch 30/100\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 12.0931 - mae: 2.4811 - val_loss: 8.4919 - val_mae: 2.2207\n",
            "Epoch 31/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 11.7413 - mae: 2.4405 - val_loss: 8.0211 - val_mae: 2.1322\n",
            "Epoch 32/100\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 11.5740 - mae: 2.4156 - val_loss: 7.8717 - val_mae: 2.1454\n",
            "Epoch 33/100\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 11.3474 - mae: 2.4135 - val_loss: 7.8042 - val_mae: 2.1398\n",
            "Epoch 34/100\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 11.0959 - mae: 2.3713 - val_loss: 7.4217 - val_mae: 2.0734\n",
            "Epoch 35/100\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 10.9596 - mae: 2.3737 - val_loss: 7.3433 - val_mae: 2.0837\n",
            "Epoch 36/100\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 10.7198 - mae: 2.3316 - val_loss: 7.1417 - val_mae: 2.0241\n",
            "Epoch 37/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 10.5979 - mae: 2.2922 - val_loss: 6.9566 - val_mae: 2.0082\n",
            "Epoch 38/100\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 10.3957 - mae: 2.3172 - val_loss: 7.0368 - val_mae: 2.0631\n",
            "Epoch 39/100\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 10.3547 - mae: 2.2741 - val_loss: 6.5477 - val_mae: 1.9168\n",
            "Epoch 40/100\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 10.0777 - mae: 2.2517 - val_loss: 6.6025 - val_mae: 1.9539\n",
            "Epoch 41/100\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 10.1027 - mae: 2.3020 - val_loss: 6.7078 - val_mae: 1.9819\n",
            "Epoch 42/100\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 9.8148 - mae: 2.2442 - val_loss: 6.3285 - val_mae: 1.8786\n",
            "Epoch 43/100\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 9.7729 - mae: 2.2198 - val_loss: 6.1036 - val_mae: 1.8404\n",
            "Epoch 44/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 9.6771 - mae: 2.2024 - val_loss: 6.1848 - val_mae: 1.8671\n",
            "Epoch 45/100\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 9.5682 - mae: 2.2023 - val_loss: 6.2001 - val_mae: 1.8712\n",
            "Epoch 46/100\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 9.4163 - mae: 2.2040 - val_loss: 6.2764 - val_mae: 1.9100\n",
            "Epoch 47/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 9.4123 - mae: 2.2133 - val_loss: 6.1639 - val_mae: 1.8663\n",
            "Epoch 48/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 9.2981 - mae: 2.1778 - val_loss: 6.0394 - val_mae: 1.8461\n",
            "Epoch 49/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 9.1419 - mae: 2.1640 - val_loss: 5.9317 - val_mae: 1.8504\n",
            "Epoch 50/100\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 9.0703 - mae: 2.1583 - val_loss: 5.9170 - val_mae: 1.8303\n",
            "Epoch 51/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 9.1246 - mae: 2.1358 - val_loss: 5.7781 - val_mae: 1.7715\n",
            "Epoch 52/100\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 8.9591 - mae: 2.1604 - val_loss: 6.6255 - val_mae: 1.9493\n",
            "Epoch 53/100\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 8.9763 - mae: 2.1392 - val_loss: 6.2015 - val_mae: 1.8366\n",
            "Epoch 54/100\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 8.6953 - mae: 2.1160 - val_loss: 6.6580 - val_mae: 1.9257\n",
            "Epoch 55/100\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 8.9478 - mae: 2.1937 - val_loss: 7.0533 - val_mae: 1.9373\n",
            "Epoch 56/100\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 9.0870 - mae: 2.1719 - val_loss: 5.9930 - val_mae: 1.7952\n",
            "Epoch 57/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 8.8585 - mae: 2.1386 - val_loss: 7.1211 - val_mae: 1.9647\n",
            "Epoch 58/100\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 8.4413 - mae: 2.1028 - val_loss: 6.2455 - val_mae: 1.8298\n",
            "Epoch 59/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 8.4322 - mae: 2.0727 - val_loss: 5.9384 - val_mae: 1.7811\n",
            "Epoch 60/100\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 8.3195 - mae: 2.0640 - val_loss: 6.7365 - val_mae: 1.9154\n",
            "Epoch 61/100\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 8.4426 - mae: 2.1210 - val_loss: 7.1998 - val_mae: 1.9686\n",
            "Epoch 62/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 8.3857 - mae: 2.0876 - val_loss: 5.6613 - val_mae: 1.7575\n",
            "Epoch 63/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 8.2166 - mae: 2.0592 - val_loss: 6.4985 - val_mae: 1.8312\n",
            "Epoch 64/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 8.0959 - mae: 2.0480 - val_loss: 6.4959 - val_mae: 1.8424\n",
            "Epoch 65/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 8.0296 - mae: 2.0504 - val_loss: 6.3662 - val_mae: 1.8415\n",
            "Epoch 66/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 8.1750 - mae: 2.0412 - val_loss: 6.5898 - val_mae: 1.8628\n",
            "Epoch 67/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 7.9551 - mae: 2.0548 - val_loss: 7.7309 - val_mae: 1.9825\n",
            "Epoch 68/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 7.9703 - mae: 2.0491 - val_loss: 6.8406 - val_mae: 1.8357\n",
            "Epoch 69/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 7.8813 - mae: 2.0155 - val_loss: 6.7434 - val_mae: 1.8293\n",
            "Epoch 70/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 7.8201 - mae: 2.0236 - val_loss: 7.9533 - val_mae: 1.9454\n",
            "Epoch 71/100\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 7.8038 - mae: 1.9968 - val_loss: 6.8508 - val_mae: 1.8518\n",
            "Epoch 72/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 7.7776 - mae: 2.0062 - val_loss: 7.6350 - val_mae: 1.9099\n",
            "Epoch 73/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 7.7651 - mae: 2.0030 - val_loss: 6.5724 - val_mae: 1.8287\n",
            "Epoch 74/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 7.6117 - mae: 1.9713 - val_loss: 6.6012 - val_mae: 1.8123\n",
            "Epoch 75/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 7.5855 - mae: 1.9908 - val_loss: 6.9715 - val_mae: 1.8225\n",
            "Epoch 76/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 7.5617 - mae: 1.9729 - val_loss: 6.9446 - val_mae: 1.8485\n",
            "Epoch 77/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 7.4835 - mae: 1.9750 - val_loss: 7.4626 - val_mae: 1.9459\n",
            "Epoch 78/100\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 7.5394 - mae: 1.9792 - val_loss: 6.8036 - val_mae: 1.8221\n",
            "Epoch 79/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 7.3713 - mae: 1.9344 - val_loss: 6.5281 - val_mae: 1.7950\n",
            "Epoch 80/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 7.4804 - mae: 1.9704 - val_loss: 6.8372 - val_mae: 1.8133\n",
            "Epoch 81/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 7.4478 - mae: 1.9578 - val_loss: 6.7174 - val_mae: 1.8120\n",
            "Epoch 82/100\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 7.4317 - mae: 2.0033 - val_loss: 8.5874 - val_mae: 1.9516\n",
            "Epoch 83/100\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 7.2000 - mae: 1.9425 - val_loss: 6.8658 - val_mae: 1.7860\n",
            "Epoch 84/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 7.4416 - mae: 1.9473 - val_loss: 7.4560 - val_mae: 1.8647\n",
            "Epoch 85/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 7.1209 - mae: 1.9313 - val_loss: 7.8770 - val_mae: 1.8895\n",
            "Epoch 86/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 7.1208 - mae: 1.9380 - val_loss: 7.0369 - val_mae: 1.7735\n",
            "Epoch 87/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 7.1956 - mae: 1.9401 - val_loss: 7.1150 - val_mae: 1.8018\n",
            "Epoch 88/100\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 7.0504 - mae: 1.8937 - val_loss: 6.5051 - val_mae: 1.7219\n",
            "Epoch 89/100\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 7.2255 - mae: 1.9706 - val_loss: 7.4082 - val_mae: 1.8330\n",
            "Epoch 90/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 7.2194 - mae: 1.9466 - val_loss: 7.1493 - val_mae: 1.7950\n",
            "Epoch 91/100\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 6.9825 - mae: 1.8868 - val_loss: 7.8038 - val_mae: 1.8805\n",
            "Epoch 92/100\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 7.1501 - mae: 1.9807 - val_loss: 8.2392 - val_mae: 1.8666\n",
            "Epoch 93/100\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 6.9348 - mae: 1.8984 - val_loss: 7.0361 - val_mae: 1.7444\n",
            "Epoch 94/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 6.8210 - mae: 1.8845 - val_loss: 8.9174 - val_mae: 1.9411\n",
            "Epoch 95/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 6.7869 - mae: 1.9009 - val_loss: 6.9720 - val_mae: 1.7630\n",
            "Epoch 96/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 6.7603 - mae: 1.8583 - val_loss: 7.5955 - val_mae: 1.8068\n",
            "Epoch 97/100\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 6.6676 - mae: 1.8637 - val_loss: 7.5699 - val_mae: 1.7905\n",
            "Epoch 98/100\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 6.6399 - mae: 1.8598 - val_loss: 7.3985 - val_mae: 1.8025\n",
            "Epoch 99/100\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 6.6564 - mae: 1.8573 - val_loss: 7.7173 - val_mae: 1.7911\n",
            "Epoch 100/100\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 6.6590 - mae: 1.8663 - val_loss: 7.9132 - val_mae: 1.8713\n"
          ]
        }
      ],
      "source": [
        "history = model.fit(X_train,Y_train,epochs=100,validation_split=0.05,verbose=1)\n",
        "\n",
        "# Epochs represents the number of times the entire training dataset will be passed forward and backward through the neural network\n",
        "# validation_split is the fraction of data to be used as validation data from the end of training data\n",
        "# For example if you have 1000 samples\n",
        "# and validation_split is 0.05, 50 samples from the end will be used for validation and remaining 950 for training.\n",
        "# Verbose field is to decide whether or not to show the progress bars"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6OuEdiMXaHgc",
        "outputId": "c0d04dd4-6e11-42e2-b987-7f76b598fd0e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "4/4 [==============================] - 0s 3ms/step\n"
          ]
        }
      ],
      "source": [
        "Y_pred = model.predict(x=X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hE3hv91HaLT9",
        "outputId": "9e7d9197-ce0b-49f5-d8e2-d573cee6738c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Predicted : [19.009132], Actual : [15.]\n",
            "Predicted : [27.436731], Actual : [26.6]\n",
            "Predicted : [43.518715], Actual : [45.4]\n",
            "Predicted : [19.87408], Actual : [20.8]\n",
            "Predicted : [31.250431], Actual : [34.9]\n",
            "Predicted : [50.07557], Actual : [21.9]\n",
            "Predicted : [26.019737], Actual : [28.7]\n",
            "Predicted : [8.766456], Actual : [7.2]\n",
            "Predicted : [19.561232], Actual : [20.]\n",
            "Predicted : [33.071377], Actual : [32.2]\n",
            "Predicted : [22.124668], Actual : [24.1]\n",
            "Predicted : [20.0587], Actual : [18.5]\n",
            "Predicted : [14.527017], Actual : [13.5]\n",
            "Predicted : [29.707294], Actual : [27.]\n",
            "Predicted : [18.018066], Actual : [23.1]\n",
            "Predicted : [18.999386], Actual : [18.9]\n",
            "Predicted : [19.395557], Actual : [24.5]\n",
            "Predicted : [38.669365], Actual : [43.1]\n",
            "Predicted : [17.025154], Actual : [19.8]\n",
            "Predicted : [17.040472], Actual : [13.8]\n",
            "Predicted : [12.748427], Actual : [15.6]\n",
            "Predicted : [30.437801], Actual : [50.]\n",
            "Predicted : [35.00664], Actual : [37.2]\n",
            "Predicted : [39.58142], Actual : [46.]\n",
            "Predicted : [42.844913], Actual : [50.]\n",
            "Predicted : [23.393545], Actual : [21.2]\n",
            "Predicted : [13.595439], Actual : [14.9]\n",
            "Predicted : [19.415384], Actual : [19.6]\n",
            "Predicted : [18.8824], Actual : [19.4]\n",
            "Predicted : [17.711494], Actual : [18.6]\n",
            "Predicted : [24.83356], Actual : [26.5]\n",
            "Predicted : [32.26355], Actual : [32.]\n",
            "Predicted : [11.672968], Actual : [10.9]\n",
            "Predicted : [20.23758], Actual : [20.]\n",
            "Predicted : [21.229397], Actual : [21.4]\n",
            "Predicted : [34.443787], Actual : [31.]\n",
            "Predicted : [25.402826], Actual : [25.]\n",
            "Predicted : [12.478526], Actual : [15.4]\n",
            "Predicted : [16.287434], Actual : [13.1]\n",
            "Predicted : [42.602097], Actual : [37.6]\n",
            "Predicted : [29.384718], Actual : [37.]\n",
            "Predicted : [18.86178], Actual : [18.9]\n",
            "Predicted : [25.302212], Actual : [27.9]\n",
            "Predicted : [46.582344], Actual : [50.]\n",
            "Predicted : [24.415445], Actual : [14.4]\n",
            "Predicted : [25.57613], Actual : [22.]\n",
            "Predicted : [18.232204], Actual : [19.9]\n",
            "Predicted : [21.378277], Actual : [21.6]\n",
            "Predicted : [16.49361], Actual : [15.6]\n",
            "Predicted : [22.610954], Actual : [15.]\n",
            "Predicted : [32.920235], Actual : [32.4]\n",
            "Predicted : [25.256731], Actual : [29.6]\n",
            "Predicted : [17.914404], Actual : [20.4]\n",
            "Predicted : [10.969855], Actual : [12.3]\n",
            "Predicted : [21.090874], Actual : [19.1]\n",
            "Predicted : [14.102272], Actual : [14.9]\n",
            "Predicted : [13.285134], Actual : [17.8]\n",
            "Predicted : [10.781698], Actual : [8.8]\n",
            "Predicted : [35.530704], Actual : [35.4]\n",
            "Predicted : [13.339529], Actual : [11.5]\n",
            "Predicted : [18.371326], Actual : [19.6]\n",
            "Predicted : [20.527288], Actual : [20.6]\n",
            "Predicted : [12.725451], Actual : [15.6]\n",
            "Predicted : [19.268826], Actual : [19.9]\n",
            "Predicted : [21.777412], Actual : [23.3]\n",
            "Predicted : [21.75546], Actual : [22.3]\n",
            "Predicted : [23.535519], Actual : [24.8]\n",
            "Predicted : [22.533268], Actual : [16.1]\n",
            "Predicted : [25.397274], Actual : [22.8]\n",
            "Predicted : [26.620394], Actual : [30.5]\n",
            "Predicted : [20.177418], Actual : [20.4]\n",
            "Predicted : [24.706762], Actual : [24.4]\n",
            "Predicted : [15.839859], Actual : [16.6]\n",
            "Predicted : [26.339779], Actual : [26.2]\n",
            "Predicted : [15.650931], Actual : [16.4]\n",
            "Predicted : [18.555126], Actual : [20.1]\n",
            "Predicted : [10.898404], Actual : [13.9]\n",
            "Predicted : [21.070974], Actual : [19.4]\n",
            "Predicted : [25.715916], Actual : [22.8]\n",
            "Predicted : [13.587217], Actual : [13.8]\n",
            "Predicted : [31.101547], Actual : [31.6]\n",
            "Predicted : [6.8496842], Actual : [10.5]\n",
            "Predicted : [22.873463], Actual : [23.8]\n",
            "Predicted : [21.633093], Actual : [22.4]\n",
            "Predicted : [19.458208], Actual : [19.3]\n",
            "Predicted : [23.3922], Actual : [22.2]\n",
            "Predicted : [14.573543], Actual : [12.6]\n",
            "Predicted : [18.506792], Actual : [19.4]\n",
            "Predicted : [19.488115], Actual : [22.2]\n",
            "Predicted : [28.35762], Actual : [29.8]\n",
            "Predicted : [11.829113], Actual : [9.6]\n",
            "Predicted : [34.245987], Actual : [34.9]\n",
            "Predicted : [22.265598], Actual : [21.4]\n",
            "Predicted : [26.305397], Actual : [25.3]\n",
            "Predicted : [32.708687], Actual : [32.9]\n",
            "Predicted : [31.976599], Actual : [26.6]\n",
            "Predicted : [15.971763], Actual : [14.6]\n",
            "Predicted : [31.77409], Actual : [31.5]\n",
            "Predicted : [28.355665], Actual : [23.3]\n",
            "Predicted : [37.577465], Actual : [33.3]\n",
            "Predicted : [20.718819], Actual : [17.5]\n",
            "Predicted : [14.415025], Actual : [19.1]\n",
            "Predicted : [47.893997], Actual : [48.5]\n",
            "Predicted : [11.518716], Actual : [17.1]\n",
            "Predicted : [24.490364], Actual : [23.1]\n",
            "Predicted : [27.31828], Actual : [28.4]\n",
            "Predicted : [15.506037], Actual : [18.9]\n",
            "Predicted : [12.23792], Actual : [13.]\n",
            "Predicted : [19.50106], Actual : [17.2]\n",
            "Predicted : [22.495117], Actual : [24.1]\n",
            "Predicted : [22.284365], Actual : [18.5]\n",
            "Predicted : [24.792763], Actual : [21.8]\n",
            "Predicted : [14.709313], Actual : [13.3]\n",
            "Predicted : [19.548782], Actual : [23.]\n",
            "Predicted : [17.13969], Actual : [14.1]\n",
            "Predicted : [20.911015], Actual : [23.9]\n",
            "Predicted : [25.45971], Actual : [24.]\n",
            "Predicted : [13.142003], Actual : [17.2]\n",
            "Predicted : [21.261433], Actual : [21.5]\n",
            "Predicted : [18.249388], Actual : [19.1]\n",
            "Predicted : [26.101406], Actual : [20.8]\n",
            "Predicted : [36.43836], Actual : [36.]\n",
            "Predicted : [22.41381], Actual : [20.1]\n",
            "Predicted : [8.575146], Actual : [8.7]\n",
            "Predicted : [17.231651], Actual : [13.6]\n",
            "Predicted : [21.636135], Actual : [22.]\n",
            "Predicted : [20.644875], Actual : [22.2]\n"
          ]
        }
      ],
      "source": [
        "for pred, actual in zip(Y_pred,Y_test.values):\n",
        "  print(f\"Predicted : {pred}, Actual : {actual}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DJkQTCTnaNDM",
        "outputId": "572adb05-68ca-49ce-a693-cb4e068be18a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "4/4 [==============================] - 0s 3ms/step - loss: 17.5897 - mae: 2.5408\n",
            "Mean Absolute Error in Prediction :  2.5407729148864746\n",
            "Mean Squared Error in Prediction :  17.589706420898438\n"
          ]
        }
      ],
      "source": [
        "mse, mae = model.evaluate(X_test,Y_test)\n",
        "\n",
        "print(\"Mean Absolute Error in Prediction : \", mae)\n",
        "print(\"Mean Squared Error in Prediction : \", mse)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
